{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同庄老师课堂所说\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "from torch.nn import Module, Linear, Dropout, LayerNorm, Identity\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okk\n"
     ]
    }
   ],
   "source": [
    "def adjust_learning_rate(optimizer, epoch ):\n",
    "    import math\n",
    "    lr = 1e-4\n",
    "    Epochs = 200\n",
    "    warmup = 5\n",
    "\n",
    "    if epoch < warmup:\n",
    "        lr = lr / (warmup - epoch)\n",
    "    else:\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (Epochs - warmup)))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "print('okk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(batch,alpha=1):\n",
    "    inputs,labels = batch\n",
    "    indices = torch.randperm(inputs.size(0))\n",
    "    shuffled_inputs = inputs[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "\n",
    "    weight,height= inputs.shape[2:]\n",
    "    lamb = np.random.beta(alpha, alpha)\n",
    "\n",
    "    rx=np.random.uniform(0, weight)\n",
    "    ry=np.random.uniform(0, height)\n",
    "\n",
    "    rw=weight*np.sqrt(1-lamb)\n",
    "    rh=height*np.sqrt(1-lamb)\n",
    "\n",
    "    x0 = int(np.round(max(rx - rw / 2, 0)))\n",
    "    x1 = int(np.round(min(rx + rw / 2, weight)))\n",
    "    y0 = int(np.round(max(ry - rh / 2, 0)))\n",
    "    y1 = int(np.round(min(ry + rh / 2, height)))\n",
    "\n",
    "    inputs[:, :, y0:y1, x0:x1] = shuffled_inputs[:, :, y0:y1, x0:x1]\n",
    "    labels = (labels, shuffled_labels, lamb)\n",
    "\n",
    "    return inputs,labels\n",
    "\n",
    "class CutMixCollator:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
    "        batch = cutmix(batch, self.alpha)\n",
    "        return batch\n",
    "\n",
    "class CutMixCriterion:\n",
    "    def __init__(self, reduction):\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        targets1, targets2, lam = targets\n",
    "        return lam * self.criterion(\n",
    "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size=128,num_workers=4,cutmix=True,alpha=1):\n",
    "    if cutmix==True:\n",
    "        collator = CutMixCollator(alpha)\n",
    "    else:\n",
    "        collator = torch.utils.data.dataloader.default_collate\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    train_dataset = datasets.CIFAR10(root='cifar', train=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(32, 4),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,]), download=True)\n",
    "      \n",
    "    dev_dataset=datasets.CIFAR10(root='cifar', train=False, transform=transforms.Compose([                                                                    \n",
    "                transforms.ToTensor(),\n",
    "                normalize,]))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,      \n",
    "                       batch_size=batch_size, shuffle=True,\n",
    "                       num_workers=num_workers, pin_memory=True,\n",
    "                       drop_last=True,collate_fn=collator)\n",
    "    \n",
    "    dev_dataloader = torch.utils.data.DataLoader( dev_dataset, \n",
    "                     batch_size=batch_size, shuffle=False,\n",
    "                     num_workers=num_workers, pin_memory=True,\n",
    "                     drop_last=False)\n",
    "    return train_dataloader,dev_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,dev_dataloader=get_loader(batch_size=128,num_workers=4,cutmix=True,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okk\n"
     ]
    }
   ],
   "source": [
    "# *------------------- CCT: Transformer -------------------------*\n",
    "# Meta CNN, CCT transformer\n",
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = Dropout(attention_dropout)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # 3, Batch , num_heads , N(seq_len) , channels // num_heads (scale)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    \"\"\"\n",
    "    Inspired by torch.nn.TransformerEncoderLayer and\n",
    "    rwightman's timm package.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = LayerNorm(d_model)\n",
    "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
    "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(Module):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_conv_layers=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 in_planes=64,\n",
    "                 activation=None,\n",
    "                 max_pool=True):\n",
    "        super(Tokenizer, self).__init__()\n",
    "        # 不用麻烦了，就搞一层\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
    "                        [n_output_channels]\n",
    "        # input -- 中间过渡 in_planes * in_planes -- output channel\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
    "                          kernel_size=(kernel_size, kernel_size),\n",
    "                          stride=(stride, stride),\n",
    "                          padding=(padding, padding), bias=False),\n",
    "                nn.Identity() if activation is None else activation(),\n",
    "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
    "                             stride=pooling_stride,\n",
    "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
    "            )\n",
    "                for i in range(n_conv_layers)\n",
    "            ])\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=32, width=32):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]  # 你这，有点不优异，自己又跑了一遍可还行\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
    "        out = self.conv_layers(x)  # 连着对 H、W downsample两次， 砍成1/4\n",
    "        return self.flattener(out).transpose(-2, -1)  # [BS , downsample(H*W) , output C]\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout_rate=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth_rate=0.1,\n",
    "                 positional_embedding='sine',\n",
    "                 sequence_length=None,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seq_pool = seq_pool\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        if not seq_pool:\n",
    "            sequence_length += 1\n",
    "            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
    "                                          requires_grad=True)\n",
    "        else:\n",
    "            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                   requires_grad=True)\n",
    "                nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = nn.Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                   requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])  # 几层 transformer\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        # transformers 堆叠\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "\n",
    "class CCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 *args, **kwargs):\n",
    "        super(CCT, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   pooling_kernel_size=pooling_kernel_size,\n",
    "                                   pooling_stride=pooling_stride,\n",
    "                                   pooling_padding=pooling_padding,\n",
    "                                   max_pool=True,\n",
    "                                   activation=nn.ReLU,  # 起码三件套得整一手吧\n",
    "                                   n_conv_layers=n_conv_layers)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=True,\n",
    "            dropout_rate=0.,\n",
    "            attention_dropout=0.1,\n",
    "            stochastic_depth=0.1,\n",
    "            *args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "print('okk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() \n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = ResNet(BasicBlock, [2,2,2,2], 10)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCT(\n",
       "  (tokenizer): Tokenizer(\n",
       "    (conv_layers): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (flattener): Flatten(start_dim=2, end_dim=3)\n",
       "  )\n",
       "  (classifier): TransformerClassifier(\n",
       "    (attention_pool): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CCT(num_layers= 7,\n",
    "        num_heads= 4,\n",
    "        mlp_ratio= 2,\n",
    "        embedding_dim= 256,\n",
    "        kernel_size=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/python_gpu/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3,weight_decay=3e-2)\n",
    "# scheduler = CosineAnnealingLR(optimizer, 200, 0)\n",
    "criterion1 = CutMixCriterion(reduction='mean')\n",
    "criterion2 = nn.CrossEntropyLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingloss=[]\n",
    "trainingacc=[]\n",
    "valloss=[]\n",
    "valacc=[]\n",
    "valacc5=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:55:11,277][<ipython-input-30-b5afce94ecf9>][line:2][INFO] start training!\n",
      "[2021-06-19 00:55:11,277][<ipython-input-30-b5afce94ecf9>][line:2][INFO] start training!\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger('transformer终稿3/exp.log')\n",
    "logger.info('start training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_25.600%:  Training average Loss: 5.593862\n",
      "Epoch 0_51.200%:  Training average Loss: 4.896263\n",
      "Epoch 0_76.800%:  Training average Loss: 4.410850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:55:44,389][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 0 :  Training average Loss: 4.078948, Training accuracy: 10.344542%,Total Time:32.641085\n",
      "[2021-06-19 00:55:44,389][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 0 :  Training average Loss: 4.078948, Training accuracy: 10.344542%,Total Time:32.641085\n",
      "[2021-06-19 00:55:46,580][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 0 :  Verification average Loss: 2.788231, Verification accuracy: 16.080000%,Verification 5 accuracy: 62.030000%,Total Time:34.832413\n",
      "[2021-06-19 00:55:46,580][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 0 :  Verification average Loss: 2.788231, Verification accuracy: 16.080000%,Verification 5 accuracy: 62.030000%,Total Time:34.832413\n",
      "[2021-06-19 00:55:47,985][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_0_accuracy_0.160800\n",
      "[2021-06-19 00:55:47,985][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_0_accuracy_0.160800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1_25.600%:  Training average Loss: 2.671988\n",
      "Epoch 1_51.200%:  Training average Loss: 2.574665\n",
      "Epoch 1_76.800%:  Training average Loss: 2.507483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:56:19,786][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 1 :  Training average Loss: 2.464810, Training accuracy: 15.715503%,Total Time:68.037573\n",
      "[2021-06-19 00:56:19,786][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 1 :  Training average Loss: 2.464810, Training accuracy: 15.715503%,Total Time:68.037573\n",
      "[2021-06-19 00:56:21,967][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 1 :  Verification average Loss: 2.194917, Verification accuracy: 20.440000%,Verification 5 accuracy: 76.470000%,Total Time:70.218940\n",
      "[2021-06-19 00:56:21,967][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 1 :  Verification average Loss: 2.194917, Verification accuracy: 20.440000%,Verification 5 accuracy: 76.470000%,Total Time:70.218940\n",
      "[2021-06-19 00:56:23,374][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_1_accuracy_0.204400\n",
      "[2021-06-19 00:56:23,374][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_1_accuracy_0.204400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2_25.600%:  Training average Loss: 2.269294\n",
      "Epoch 2_51.200%:  Training average Loss: 2.248110\n",
      "Epoch 2_76.800%:  Training average Loss: 2.227944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:56:55,774][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 2 :  Training average Loss: 2.216511, Training accuracy: 18.734846%,Total Time:104.026144\n",
      "[2021-06-19 00:56:55,774][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 2 :  Training average Loss: 2.216511, Training accuracy: 18.734846%,Total Time:104.026144\n",
      "[2021-06-19 00:56:57,944][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 2 :  Verification average Loss: 1.962278, Verification accuracy: 27.630000%,Verification 5 accuracy: 83.900000%,Total Time:106.195740\n",
      "[2021-06-19 00:56:57,944][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 2 :  Verification average Loss: 1.962278, Verification accuracy: 27.630000%,Verification 5 accuracy: 83.900000%,Total Time:106.195740\n",
      "[2021-06-19 00:56:59,362][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_2_accuracy_0.276300\n",
      "[2021-06-19 00:56:59,362][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_2_accuracy_0.276300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3_25.600%:  Training average Loss: 2.127795\n",
      "Epoch 3_51.200%:  Training average Loss: 2.113960\n",
      "Epoch 3_76.800%:  Training average Loss: 2.107398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:57:31,378][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 3 :  Training average Loss: 2.100089, Training accuracy: 22.581344%,Total Time:139.630421\n",
      "[2021-06-19 00:57:31,378][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 3 :  Training average Loss: 2.100089, Training accuracy: 22.581344%,Total Time:139.630421\n",
      "[2021-06-19 00:57:33,628][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 3 :  Verification average Loss: 1.762428, Verification accuracy: 36.130000%,Verification 5 accuracy: 88.910000%,Total Time:141.880022\n",
      "[2021-06-19 00:57:33,628][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 3 :  Verification average Loss: 1.762428, Verification accuracy: 36.130000%,Verification 5 accuracy: 88.910000%,Total Time:141.880022\n",
      "[2021-06-19 00:57:35,041][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_3_accuracy_0.361300\n",
      "[2021-06-19 00:57:35,041][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_3_accuracy_0.361300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4_25.600%:  Training average Loss: 2.029927\n",
      "Epoch 4_51.200%:  Training average Loss: 2.020802\n",
      "Epoch 4_76.800%:  Training average Loss: 2.018031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-19 00:58:06,884][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 4 :  Training average Loss: 2.013888, Training accuracy: 26.813588%,Total Time:175.136029\n",
      "[2021-06-19 00:58:06,884][<ipython-input-31-48b2ae7a8075>][line:51][INFO] Epoch 4 :  Training average Loss: 2.013888, Training accuracy: 26.813588%,Total Time:175.136029\n",
      "[2021-06-19 00:58:09,912][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 4 :  Verification average Loss: 1.520676, Verification accuracy: 46.440000%,Verification 5 accuracy: 92.690000%,Total Time:178.163906\n",
      "[2021-06-19 00:58:09,912][<ipython-input-31-48b2ae7a8075>][line:92][INFO] Epoch 4 :  Verification average Loss: 1.520676, Verification accuracy: 46.440000%,Verification 5 accuracy: 92.690000%,Total Time:178.163906\n",
      "[2021-06-19 00:58:13,572][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_4_accuracy_0.464400\n",
      "[2021-06-19 00:58:13,572][<ipython-input-31-48b2ae7a8075>][line:102][INFO] Model is saved in transformer终稿3/epoch_4_accuracy_0.464400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch=300\n",
    "best_accuracy=0.0\n",
    "es=0\n",
    "start_time=time.time()\n",
    "for i in range(epoch):\n",
    "    adjust_learning_rate(optimizer , i)\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_dataloader.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    for batch in train_dataloader:\n",
    "        steps+=1\n",
    "        optimizer.zero_grad() \n",
    "        # 取数据\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels, shuffled_labels, lamb=labels\n",
    "        labels=(labels.to(device), shuffled_labels.to(device), lamb)\n",
    "        #targets1, targets2, lam = targets\n",
    "        #, labels.to(device)  # 将输入和目标在每一步都送入GPU\n",
    "        outputs = model(inputs)\n",
    "        #_, outputs = torch.max(outputs.data, 1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion1(outputs, labels).to(device)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        total_loss = total_loss + loss.item() \n",
    "\n",
    "        #计算正确率\n",
    "        labels, shuffled_labels, lamb=labels\n",
    "        correct1 = (torch.max(outputs, dim=1)[1]  #get the indices\n",
    "                   .view(labels.size()) == labels).sum()\n",
    "        correct2 = (torch.max(outputs, dim=1)[1]  #get the indices\n",
    "                   .view(shuffled_labels.size()) == shuffled_labels).sum()\n",
    "\n",
    "        correct = (lamb * correct1.item() + (1 - lamb) * correct2.item()) \n",
    "\n",
    "        total_correct = total_correct + correct\n",
    "\n",
    " \n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))\n",
    "    logger.info(\"Epoch %d :  Training average Loss: %f, Training accuracy: %f%%,Total Time:%f\"\n",
    "      %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))   \n",
    "    trainingloss.append(total_loss/steps)\n",
    "    trainingacc.append(total_correct/total_data_num) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #验证\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_correctk=0.0\n",
    "    total_data_num = len(dev_dataloader.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in dev_dataloader:\n",
    "        steps+=1\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion2(outputs, labels)  \n",
    "        total_loss = total_loss + loss.item() \n",
    "        correct = (torch.max(outputs, dim=1)[1]  #get the indices\n",
    "                   .view(labels.size()) == labels).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        maxk = max((1,5))\n",
    "        yresize = labels.view(-1,1)\n",
    "        _, pred = outputs.topk(maxk, 1, True, True)\n",
    "\n",
    "        correctk = torch.eq(pred, yresize).sum()\n",
    "\n",
    "        #correct = (torch.max(outputs, dim=1)[1]  #get the indices\n",
    "                   #.view(labels.size()) == labels).sum()\n",
    "        total_correctk = total_correctk + correctk.item()\n",
    "        \n",
    "    logger.info(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Verification 5 accuracy: %f%%,Total Time:%f\"\n",
    "      %(i, total_loss/steps, total_correct*100/total_data_num,total_correctk*100/total_data_num,time.time()-start_time))  \n",
    "    #print(\"Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f\"\n",
    "      #%(i,  total_correctk*100/total_data_num,time.time()-start_time))  \n",
    "    valloss.append(total_loss/steps)\n",
    "    valacc.append(total_correct/total_data_num) \n",
    "    valacc5.append(total_correctk/total_data_num) \n",
    "    if best_accuracy < total_correct/total_data_num :\n",
    "        es = 0\n",
    "        best_accuracy =total_correct/total_data_num \n",
    "        torch.save(model,'transformer终稿3/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "        logger.info('Model is saved in transformer终稿3/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "        #torch.cuda.empty_cache()\n",
    "    #torch.save(model.state_dict(), 'cutmix/epoch_%d_accuracy_%f.pkl'%(i,total_correct/total_data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingloss=pd.DataFrame(data=trainingloss)\n",
    "trainingloss.to_csv('transformer终稿3/trainingloss.csv',encoding='utf-8')\n",
    "\n",
    "trainingacc=pd.DataFrame(data=trainingacc)\n",
    "trainingacc.to_csv('transformer终稿3/trainingacc.csv',encoding='utf-8')\n",
    "\n",
    "valloss=pd.DataFrame(data=valloss)\n",
    "valloss.to_csv('transformer终稿3/valloss.csv',encoding='utf-8')\n",
    "\n",
    "valacc=pd.DataFrame(data=valacc)\n",
    "valacc.to_csv('transformer终稿3/valacc.csv',encoding='utf-8')\n",
    "\n",
    "valacc5=pd.DataFrame(data=valacc5)\n",
    "valacc5.to_csv('transformer终稿3/valacc5.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('transformer终稿3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_gpu]",
   "language": "python",
   "name": "conda-env-python_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
