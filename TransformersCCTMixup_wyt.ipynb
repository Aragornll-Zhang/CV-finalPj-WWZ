{"cells":[{"cell_type":"markdown","metadata":{"id":"OnVNNp-RBBBq"},"source":["# Transformers Pipeline Work"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31956,"status":"ok","timestamp":1624287448564,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"nfK1-raXA29g","outputId":"a1c1f9a2-fb80-4ff1-8647-360642d28c41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/mydrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/mydrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1939,"status":"ok","timestamp":1624287450500,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"qxTYs2-0ShuJ","outputId":"856734a0-7c09-418f-bc68-307bb1bca00e"},"outputs":[{"name":"stdout","output_type":"stream","text":["okk\n"]}],"source":["# 同庄老师课堂所说\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n","from torch.nn import Module, Linear, Dropout, LayerNorm, Identity\n","import pandas as pd\n","import numpy as np\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","from torch import optim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import matplotlib.pyplot as plt\n","import time\n","from torch.autograd import Variable\n","use_cuda = torch.cuda.is_available()\n","print('okk')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":761,"status":"ok","timestamp":1624287451257,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"fz5QARd2BP10","outputId":"a85f4218-b974-4493-9823-40f38045c431"},"outputs":[{"name":"stdout","output_type":"stream","text":["okk\n"]}],"source":["# *------------------- CCT: Transformer -------------------------*\n","# Meta CNN, CCT transformer\n","class Attention(Module):\n","    \"\"\"\n","    Obtained from: github.com:rwightman/pytorch-image-models\n","    \"\"\"\n","\n","    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // self.num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = Linear(dim, dim * 3, bias=False)\n","        self.attn_drop = Dropout(attention_dropout)\n","        self.proj = Linear(dim, dim)\n","        self.proj_drop = Dropout(projection_dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        # 3, Batch , num_heads , N(seq_len) , channels // num_heads (scale)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class TransformerEncoderLayer(Module):\n","    \"\"\"\n","    Inspired by torch.nn.TransformerEncoderLayer and\n","    rwightman's timm package.\n","    \"\"\"\n","\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n","                 attention_dropout=0.1, drop_path_rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.pre_norm = LayerNorm(d_model)\n","        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n","                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n","\n","        self.linear1 = Linear(d_model, dim_feedforward)\n","        self.dropout1 = Dropout(dropout)\n","        self.norm1 = LayerNorm(d_model)\n","        self.linear2 = Linear(dim_feedforward, d_model)\n","        self.dropout2 = Dropout(dropout)\n","\n","        self.drop_path = DropPath(drop_path_rate) if drop_path_rate \u003e 0 else Identity()\n","\n","        self.activation = F.gelu\n","\n","    def forward(self, src: torch.Tensor, *args, **kwargs) -\u003e torch.Tensor:\n","        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n","        src = self.norm1(src)\n","        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n","        src = src + self.drop_path(self.dropout2(src2))\n","        return src\n","\n","\n","def drop_path(x, drop_prob: float = 0., training: bool = False):\n","    \"\"\"\n","    Obtained from: github.com:rwightman/pytorch-image-models\n","    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n","    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n","    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n","    'survival rate' as the argument.\n","    \"\"\"\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n","    random_tensor.floor_()  # binarize\n","    output = x.div(keep_prob) * random_tensor\n","    return output\n","\n","\n","class DropPath(Module):\n","    \"\"\"\n","    Obtained from: github.com:rwightman/pytorch-image-models\n","    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","\n","\n","class Tokenizer(nn.Module):\n","    def __init__(self,\n","                 kernel_size, stride, padding,\n","                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n","                 n_conv_layers=1,\n","                 n_input_channels=3,\n","                 n_output_channels=64,\n","                 in_planes=64,\n","                 activation=None,\n","                 max_pool=True):\n","        super(Tokenizer, self).__init__()\n","        # 不用麻烦了，就搞一层\n","        n_filter_list = [n_input_channels] + \\\n","                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n","                        [n_output_channels]\n","        # input -- 中间过渡 in_planes * in_planes -- output channel\n","\n","        self.conv_layers = nn.Sequential(\n","            *[nn.Sequential(\n","                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n","                          kernel_size=(kernel_size, kernel_size),\n","                          stride=(stride, stride),\n","                          padding=(padding, padding), bias=False),\n","                nn.Identity() if activation is None else activation(),\n","                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n","                             stride=pooling_stride,\n","                             padding=pooling_padding) if max_pool else nn.Identity()\n","            )\n","                for i in range(n_conv_layers)\n","            ])\n","\n","        self.flattener = nn.Flatten(2, 3)\n","        self.apply(self.init_weight)\n","\n","    def sequence_length(self, n_channels=3, height=32, width=32):\n","        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]  # 你这，有点不优异，自己又跑了一遍可还行\n","\n","    def forward(self, x):\n","        # return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n","        out = self.conv_layers(x)  # 连着对 H、W downsample两次， 砍成1/4\n","        return self.flattener(out).transpose(-2, -1)  # [BS , downsample(H*W) , output C]\n","\n","    @staticmethod\n","    def init_weight(m):\n","        if isinstance(m, nn.Conv2d):\n","            nn.init.kaiming_normal_(m.weight)\n","\n","\n","class TransformerClassifier(nn.Module):\n","    def __init__(self,\n","                 seq_pool=True,\n","                 embedding_dim=768,\n","                 num_layers=12,\n","                 num_heads=12,\n","                 mlp_ratio=4.0,\n","                 num_classes=1000,\n","                 dropout_rate=0.1,\n","                 attention_dropout=0.1,\n","                 stochastic_depth_rate=0.1,\n","                 positional_embedding='sine',\n","                 sequence_length=None,\n","                 *args, **kwargs):\n","        super().__init__()\n","        positional_embedding = positional_embedding if \\\n","            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n","        dim_feedforward = int(embedding_dim * mlp_ratio)\n","        self.embedding_dim = embedding_dim\n","        self.sequence_length = sequence_length\n","        self.seq_pool = seq_pool\n","\n","        assert sequence_length is not None or positional_embedding == 'none', \\\n","            f\"Positional embedding is set to {positional_embedding} and\" \\\n","            f\" the sequence length was not specified.\"\n","\n","        if not seq_pool:\n","            sequence_length += 1\n","            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim),\n","                                          requires_grad=True)\n","        else:\n","            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n","\n","        if positional_embedding != 'none':\n","            if positional_embedding == 'learnable':\n","                self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n","                                                   requires_grad=True)\n","                nn.init.trunc_normal_(self.positional_emb, std=0.2)\n","            else:\n","                self.positional_emb = nn.Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n","                                                   requires_grad=False)\n","        else:\n","            self.positional_emb = None\n","\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n","        self.blocks = nn.ModuleList([\n","            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n","                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n","                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n","            for i in range(num_layers)])  # 几层 transformer\n","        self.norm = nn.LayerNorm(embedding_dim)\n","\n","        self.fc = nn.Linear(embedding_dim, num_classes)\n","        self.apply(self.init_weight)\n","\n","    def forward(self, x):\n","        if self.positional_emb is None and x.size(1) \u003c self.sequence_length:\n","            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n","\n","        if not self.seq_pool:\n","            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n","            x = torch.cat((cls_token, x), dim=1)\n","\n","        if self.positional_emb is not None:\n","            x += self.positional_emb\n","\n","        x = self.dropout(x)\n","        # transformers 堆叠\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","\n","        if self.seq_pool:\n","            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n","        else:\n","            x = x[:, 0]\n","\n","        x = self.fc(x)\n","        return x\n","\n","    @staticmethod\n","    def init_weight(m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @staticmethod\n","    def sinusoidal_embedding(n_channels, dim):\n","        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n","                                for p in range(n_channels)])\n","        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n","        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n","        return pe.unsqueeze(0)\n","\n","\n","class CCT(nn.Module):\n","    def __init__(self,\n","                 img_size=32,\n","                 embedding_dim=768,\n","                 n_input_channels=3,\n","                 n_conv_layers=1,\n","                 kernel_size=3,\n","                 stride=2,\n","                 padding=3,\n","                 pooling_kernel_size=3,\n","                 pooling_stride=2,\n","                 pooling_padding=1,\n","                 *args, **kwargs):\n","        super(CCT, self).__init__()\n","\n","        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n","                                   n_output_channels=embedding_dim,\n","                                   kernel_size=kernel_size,\n","                                   stride=stride,\n","                                   padding=padding,\n","                                   pooling_kernel_size=pooling_kernel_size,\n","                                   pooling_stride=pooling_stride,\n","                                   pooling_padding=pooling_padding,\n","                                   max_pool=True,\n","                                   activation=nn.ReLU,  # 起码三件套得整一手吧\n","                                   n_conv_layers=n_conv_layers)\n","\n","        self.classifier = TransformerClassifier(\n","            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n","                                                           height=img_size,\n","                                                           width=img_size),\n","            embedding_dim=embedding_dim,\n","            seq_pool=True,\n","            dropout_rate=0.,\n","            attention_dropout=0.1,\n","            stochastic_depth=0.1,\n","            *args, **kwargs)\n","\n","    def forward(self, x):\n","        x = self.tokenizer(x)\n","        return self.classifier(x)\n","print('okk')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1624287451258,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"rneLLyUUBXHH","outputId":"c0a716cb-3e7e-470e-cdeb-f5cde842653f"},"outputs":[{"name":"stdout","output_type":"stream","text":["okk\n"]}],"source":["def train(model, train_X, loss_func, optimizer):\n","    model.train()\n","    total_loss = 0\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    for i, (x, y) in enumerate(train_X):\n","        x = x.to(device)\n","        y = y.to(device)\n","        outputs = model(x)\n","        loss = loss_func(outputs, y)\n","        # backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * x.shape[0]\n","        if (i + 1) % 300 == 0:\n","            print(\"Step [{}/{}] Train Loss: {:.4f}\".format(i + 1, len(train_X), loss.item()))\n","\n","    print(total_loss / len(train_X.dataset))  # 其实loss计算有点小问题\n","    return total_loss / len(train_X.dataset)\n","\n","\n","def predict(model,test_X):\n","    model.eval()\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    top1_acc = top5_acc = 0\n","    with torch.no_grad():\n","        for i , (x,y) in enumerate(test_X):\n","            x = x.to(device)\n","            y = y.to(device)\n","            output = model(x)\n","            _, indice = output.topk(5, -1, True, sorted= True)\n","            top1_acc += torch.eq(indice[:, 0], y).sum().item()\n","            top5_acc += torch.eq(indice , y.view(-1,1)).sum().item()\n","    return top1_acc/len(test_X.dataset) , top5_acc/len(test_X.dataset)\n","print('okk')"]},{"cell_type":"markdown","metadata":{"id":"-vd01c_qBsAs"},"source":["# mixup"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1624287452200,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"KIJK5ZLD91fY"},"outputs":[],"source":["mixup=True\n","def mixup_data(x, y, alpha=1.0, use_cuda=True):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha \u003e 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    if use_cuda:\n","        index = torch.randperm(batch_size).cuda()\n","    else:\n","        index = torch.randperm(batch_size)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"]},{"cell_type":"markdown","metadata":{"id":"UijiF1apB4Kq"},"source":["# dataLoader\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1624287452201,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"0TIOZ5UPB-D4"},"outputs":[],"source":["def get_loader(batch_size=128,num_workers=4,mixup=True,alpha=1):\n","    collator = torch.utils.data.dataloader.default_collate\n","\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","    train_dataset = datasets.CIFAR10(root='cifar', train=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(),\n","                    transforms.RandomCrop(32, 4),\n","                    transforms.ToTensor(),\n","                    normalize,]), download=True)\n","      \n","    dev_dataset=datasets.CIFAR10(root='cifar', train=False, transform=transforms.Compose([                                                                    \n","                transforms.ToTensor(),\n","                normalize,]))\n","\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset,      \n","                       batch_size=batch_size, shuffle=True,\n","                       num_workers=num_workers, pin_memory=True,\n","                       drop_last=True,collate_fn=collator)\n","    \n","    dev_dataloader = torch.utils.data.DataLoader( dev_dataset, \n","                     batch_size=batch_size, shuffle=False,\n","                     num_workers=num_workers, pin_memory=True,\n","                     drop_last=False)\n","    return train_dataloader,dev_dataloader"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"elapsed":7018,"status":"ok","timestamp":1624287459205,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"a8vwMfpCB_eN","outputId":"a81791da-9269-4f1a-80ce-ee09bf01d194"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0dd346a445e4835a15a6251519791d8","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Extracting cifar/cifar-10-python.tar.gz to cifar\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["train_dataloader,dev_dataloader=get_loader(batch_size=128,num_workers=4,mixup=True,alpha=1)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1624287459206,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"A00pCRR1gMF4","outputId":"466ca500-45ef-44ad-e588-61702b8f3941"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/mydrive/MyDrive/pj2/outputs/CCT7_MIXUP_66R2.pth\n"]}],"source":["date = '66R2'\n","if mixup:\n","  signal = 'CCT7_MIXUP_' + date + '.pth'\n","else:\n","  signal = 'CCT7_' + date + '.pth'\n","\n","PATH = '/content/mydrive/MyDrive/pj2/outputs/' + signal\n","print(PATH)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6428,"status":"ok","timestamp":1624287465621,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"VacCfDIzCQP9","outputId":"6d7c3e3b-8aa4-4b66-8090-9c31b8268e7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"name":"stdout","output_type":"stream","text":["refresh\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = CCT(num_layers= 7,\n","        num_heads= 4,\n","        mlp_ratio= 2,\n","        embedding_dim= 256,\n","        kernel_size=3)\n","model.to(device)\n","# copy from github\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay= 5e-4,nesterov=True)\n","# scheduler = MultiStepLR(optimizer, milestones=[60,120,160,180], gamma=0.5)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.2, momentum=0.9, weight_decay=1e-4,nesterov=True)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,200,0)\n","# scheduler = CosineAnnealingLR(optimizer, 200, 0)\n","\n","loss_func = nn.CrossEntropyLoss()\n","# scheduler = MultiStepLR(optimizer, milestones=[30,60,90,120,150,160,180], gamma=0.7)\n","print('refresh')\n","best_accuracy = None"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1624287465621,"user":{"displayName":"TONY STARK","photoUrl":"","userId":"01153164418852788847"},"user_tz":-480},"id":"ptp8-tbYJ5MY","outputId":"501ab336-9b8d-4807-8e83-71f37fbdc844"},"outputs":[{"name":"stdout","output_type":"stream","text":["okk\n"]}],"source":["def adjust_learning_rate(optimizer, epoch ):\n","    import math\n","    lr = 1e-3\n","    Epochs = 200\n","    warmup = 5\n","\n","    if epoch \u003c warmup:\n","        lr = lr / (warmup - epoch)\n","    else:\n","        lr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (Epochs - warmup)))\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","print('okk')"]},{"cell_type":"markdown","metadata":{"id":"bK7QIkq5usgq"},"source":["# train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"o3mrnLPZFYYb"},"outputs":[{"name":"stdout","output_type":"stream","text":["start to train********************\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["Step [300/390] Train Loss: 2.4002\n","3.556126202392578\n","训练轮次 0 loss: 3.556126202392578 top1 \u0026 top5: 0.2241 0.7233 总训练时间: 36.80139970779419\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_0_accuracy_0.224100\n","Step [300/390] Train Loss: 2.2201\n","2.23894413269043\n","训练轮次 1 loss: 2.23894413269043 top1 \u0026 top5: 0.2468 0.7595 总训练时间: 74.2878987789154\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_1_accuracy_0.246800\n","Step [300/390] Train Loss: 2.1808\n","2.1311169262695313\n","训练轮次 2 loss: 2.1311169262695313 top1 \u0026 top5: 0.2638 0.7727 总训练时间: 111.03606700897217\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_2_accuracy_0.263800\n","Step [300/390] Train Loss: 2.0967\n","2.0505310052490233\n","训练轮次 3 loss: 2.0505310052490233 top1 \u0026 top5: 0.2889 0.8018 总训练时间: 147.6942183971405\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_3_accuracy_0.288900\n","Step [300/390] Train Loss: 1.9535\n","1.9599160586547852\n","训练轮次 4 loss: 1.9599160586547852 top1 \u0026 top5: 0.3044 0.8361 总训练时间: 184.36888647079468\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_4_accuracy_0.304400\n","Step [300/390] Train Loss: 1.8676\n","1.8607007418823243\n","训练轮次 5 loss: 1.8607007418823243 top1 \u0026 top5: 0.3464 0.858 总训练时间: 221.00794053077698\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_5_accuracy_0.346400\n","Step [300/390] Train Loss: 1.7830\n","1.7660968832397461\n","训练轮次 6 loss: 1.7660968832397461 top1 \u0026 top5: 0.3713 0.8797 总训练时间: 257.70312428474426\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_6_accuracy_0.371300\n","Step [300/390] Train Loss: 1.6480\n","1.6874258096313477\n","训练轮次 7 loss: 1.6874258096313477 top1 \u0026 top5: 0.3833 0.8899 总训练时间: 294.5695254802704\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_7_accuracy_0.383300\n","Step [300/390] Train Loss: 1.5970\n","1.6327379794311523\n","训练轮次 8 loss: 1.6327379794311523 top1 \u0026 top5: 0.3954 0.897 总训练时间: 331.36155557632446\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_8_accuracy_0.395400\n","Step [300/390] Train Loss: 1.4674\n","1.591486480102539\n","训练轮次 9 loss: 1.591486480102539 top1 \u0026 top5: 0.4121 0.9027 总训练时间: 368.0951991081238\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_9_accuracy_0.412100\n","Step [300/390] Train Loss: 1.6109\n","1.5537061798095704\n","训练轮次 10 loss: 1.5537061798095704 top1 \u0026 top5: 0.4376 0.9137 总训练时间: 404.8324420452118\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_10_accuracy_0.437600\n","Step [300/390] Train Loss: 1.4629\n","1.5226163806152344\n","训练轮次 11 loss: 1.5226163806152344 top1 \u0026 top5: 0.447 0.9181 总训练时间: 441.54209756851196\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_11_accuracy_0.447000\n","Step [300/390] Train Loss: 1.5227\n","1.483879390563965\n","训练轮次 12 loss: 1.483879390563965 top1 \u0026 top5: 0.4555 0.9234 总训练时间: 478.4017450809479\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_12_accuracy_0.455500\n","Step [300/390] Train Loss: 1.3843\n","1.450823955078125\n","训练轮次 13 loss: 1.450823955078125 top1 \u0026 top5: 0.4679 0.9292 总训练时间: 515.2545011043549\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_13_accuracy_0.467900\n","Step [300/390] Train Loss: 1.5960\n","1.4198145434570313\n","训练轮次 14 loss: 1.4198145434570313 top1 \u0026 top5: 0.4899 0.9334 总训练时间: 552.041748046875\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_14_accuracy_0.489900\n","Step [300/390] Train Loss: 1.3861\n","1.3900204901123048\n","训练轮次 15 loss: 1.3900204901123048 top1 \u0026 top5: 0.5117 0.9406 总训练时间: 588.9265179634094\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_15_accuracy_0.511700\n","Step [300/390] Train Loss: 1.2790\n","1.367099690246582\n","训练轮次 16 loss: 1.367099690246582 top1 \u0026 top5: 0.4995 0.9382 总训练时间: 625.8518834114075\n","Step [300/390] Train Loss: 1.2796\n","1.3453741290283203\n","训练轮次 17 loss: 1.3453741290283203 top1 \u0026 top5: 0.5182 0.9399 总训练时间: 662.1540200710297\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_17_accuracy_0.518200\n","Step [300/390] Train Loss: 1.4343\n","1.3240106283569335\n","训练轮次 18 loss: 1.3240106283569335 top1 \u0026 top5: 0.5333 0.9459 总训练时间: 698.4975221157074\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_18_accuracy_0.533300\n","Step [300/390] Train Loss: 1.2290\n","1.3001587315368652\n","训练轮次 19 loss: 1.3001587315368652 top1 \u0026 top5: 0.5335 0.9462 总训练时间: 734.9189088344574\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_19_accuracy_0.533500\n","Step [300/390] Train Loss: 1.3926\n","1.281593900756836\n","训练轮次 20 loss: 1.281593900756836 top1 \u0026 top5: 0.5462 0.9503 总训练时间: 771.4543488025665\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_20_accuracy_0.546200\n","Step [300/390] Train Loss: 1.2143\n","1.2647488330078125\n","训练轮次 21 loss: 1.2647488330078125 top1 \u0026 top5: 0.5531 0.9508 总训练时间: 807.9243059158325\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_21_accuracy_0.553100\n","Step [300/390] Train Loss: 1.1814\n","1.2482642568969726\n","训练轮次 22 loss: 1.2482642568969726 top1 \u0026 top5: 0.5535 0.9494 总训练时间: 844.1348133087158\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_22_accuracy_0.553500\n","Step [300/390] Train Loss: 1.1770\n","1.2350247966003418\n","训练轮次 23 loss: 1.2350247966003418 top1 \u0026 top5: 0.5675 0.9552 总训练时间: 880.4675331115723\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_23_accuracy_0.567500\n","Step [300/390] Train Loss: 0.9962\n","1.2159959796142579\n","训练轮次 24 loss: 1.2159959796142579 top1 \u0026 top5: 0.5653 0.9549 总训练时间: 916.7049341201782\n","Step [300/390] Train Loss: 1.1085\n","1.2051263926696778\n","训练轮次 25 loss: 1.2051263926696778 top1 \u0026 top5: 0.5743 0.9533 总训练时间: 952.699982881546\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_25_accuracy_0.574300\n","Step [300/390] Train Loss: 1.0790\n","1.192013600616455\n","训练轮次 26 loss: 1.192013600616455 top1 \u0026 top5: 0.5776 0.9541 总训练时间: 988.8626034259796\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_26_accuracy_0.577600\n","Step [300/390] Train Loss: 1.1362\n","1.1808040675354003\n","训练轮次 27 loss: 1.1808040675354003 top1 \u0026 top5: 0.5839 0.9585 总训练时间: 1024.8439888954163\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_27_accuracy_0.583900\n","Step [300/390] Train Loss: 1.1692\n","1.166729270477295\n","训练轮次 28 loss: 1.166729270477295 top1 \u0026 top5: 0.5779 0.9587 总训练时间: 1061.029307603836\n","Step [300/390] Train Loss: 1.2054\n","1.1570168135070802\n","训练轮次 29 loss: 1.1570168135070802 top1 \u0026 top5: 0.5942 0.9606 总训练时间: 1097.3420045375824\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_29_accuracy_0.594200\n","Step [300/390] Train Loss: 1.1307\n","1.1432008448791504\n","训练轮次 30 loss: 1.1432008448791504 top1 \u0026 top5: 0.592 0.9605 总训练时间: 1134.064134836197\n","Step [300/390] Train Loss: 1.0321\n","1.1353758715820312\n","训练轮次 31 loss: 1.1353758715820312 top1 \u0026 top5: 0.5967 0.9614 总训练时间: 1170.4473974704742\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_31_accuracy_0.596700\n","Step [300/390] Train Loss: 1.1032\n","1.1241949305725099\n","训练轮次 32 loss: 1.1241949305725099 top1 \u0026 top5: 0.5994 0.9592 总训练时间: 1207.1052572727203\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_32_accuracy_0.599400\n","Step [300/390] Train Loss: 1.0884\n","1.1128689401245118\n","训练轮次 33 loss: 1.1128689401245118 top1 \u0026 top5: 0.5987 0.9599 总训练时间: 1243.576805114746\n","Step [300/390] Train Loss: 1.1753\n","1.1086628259277345\n","训练轮次 34 loss: 1.1086628259277345 top1 \u0026 top5: 0.5958 0.9606 总训练时间: 1280.0068759918213\n","Step [300/390] Train Loss: 1.1266\n","1.095128387145996\n","训练轮次 35 loss: 1.095128387145996 top1 \u0026 top5: 0.6158 0.9635 总训练时间: 1316.5419895648956\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_35_accuracy_0.615800\n","Step [300/390] Train Loss: 0.9422\n","1.0890508024597167\n","训练轮次 36 loss: 1.0890508024597167 top1 \u0026 top5: 0.6194 0.9621 总训练时间: 1353.1627378463745\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_36_accuracy_0.619400\n","Step [300/390] Train Loss: 1.0426\n","1.074207773590088\n","训练轮次 37 loss: 1.074207773590088 top1 \u0026 top5: 0.6186 0.9653 总训练时间: 1389.7829172611237\n","Step [300/390] Train Loss: 0.9981\n","1.07021408493042\n","训练轮次 38 loss: 1.07021408493042 top1 \u0026 top5: 0.6252 0.9648 总训练时间: 1426.330022096634\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_38_accuracy_0.625200\n","Step [300/390] Train Loss: 1.1149\n","1.0635138626098632\n","训练轮次 39 loss: 1.0635138626098632 top1 \u0026 top5: 0.6305 0.9645 总训练时间: 1463.0540721416473\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_39_accuracy_0.630500\n","Step [300/390] Train Loss: 0.9800\n","1.0525431755065917\n","训练轮次 40 loss: 1.0525431755065917 top1 \u0026 top5: 0.6248 0.9646 总训练时间: 1499.6742238998413\n","Step [300/390] Train Loss: 1.0872\n","1.0446513389587402\n","训练轮次 41 loss: 1.0446513389587402 top1 \u0026 top5: 0.6411 0.9682 总训练时间: 1536.1117868423462\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_41_accuracy_0.641100\n","Step [300/390] Train Loss: 1.1324\n","1.0374476277160645\n","训练轮次 42 loss: 1.0374476277160645 top1 \u0026 top5: 0.628 0.9669 总训练时间: 1572.556722164154\n","Step [300/390] Train Loss: 0.8445\n","1.030091582183838\n","训练轮次 43 loss: 1.030091582183838 top1 \u0026 top5: 0.634 0.9664 总训练时间: 1608.8687794208527\n","Step [300/390] Train Loss: 1.1288\n","1.021349870147705\n","训练轮次 44 loss: 1.021349870147705 top1 \u0026 top5: 0.6391 0.9678 总训练时间: 1645.2955663204193\n","Step [300/390] Train Loss: 1.0219\n","1.0179560952758788\n","训练轮次 45 loss: 1.0179560952758788 top1 \u0026 top5: 0.6497 0.9689 总训练时间: 1681.7222344875336\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_45_accuracy_0.649700\n","Step [300/390] Train Loss: 0.9202\n","1.0076516749572755\n","训练轮次 46 loss: 1.0076516749572755 top1 \u0026 top5: 0.6427 0.9699 总训练时间: 1718.3709213733673\n","Step [300/390] Train Loss: 1.1559\n","1.00212370513916\n","训练轮次 47 loss: 1.00212370513916 top1 \u0026 top5: 0.6465 0.9686 总训练时间: 1754.7495999336243\n","Step [300/390] Train Loss: 1.0749\n","0.9977869502258301\n","训练轮次 48 loss: 0.9977869502258301 top1 \u0026 top5: 0.6517 0.9716 总训练时间: 1791.1140949726105\n","Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_48_accuracy_0.651700\n"]}],"source":["import time\n","start = time.time()\n","Iter_Time = 200\n","loss_train = np.zeros(Iter_Time)\n","test_acc= np.zeros(Iter_Time)\n","test_5acc= np.zeros(Iter_Time)\n","train_acc = np.zeros(Iter_Time)\n","print('start to train' + '*'*20)\n","if best_accuracy is None:\n","  best_accuracy = 0   \n","for epoch in range(Iter_Time):\n","    adjust_learning_rate(optimizer , epoch)\n","    loss = train(model= model, train_X=train_dataloader, loss_func=loss_func,optimizer=optimizer)\n","    loss_train[epoch] = loss\n","    top1 , top5 = predict(model=model, test_X =dev_dataloader)\n","    test_acc[epoch] = top1\n","    test_5acc[epoch] = top5\n","    print('训练轮次',epoch, 'loss:' , loss ,'top1 \u0026 top5:' ,top1 , top5 , '总训练时间:', time.time() - start)\n","   \n","\n","    if best_accuracy \u003c top1:\n","      best_accuracy = top1 \n","      torch.save(model,'/content/mydrive/MyDrive/pj2/outputs/epoch_%d_accuracy_%f'%(epoch,top1))\n","      print('Model is saved in /content/mydrive/MyDrive/pj2/outputs/epoch_%d_accuracy_%f'%(epoch,top1))\n","    if top1 \u003c 0.15: break\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eT5ertjfECSQ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWbtgGSJLC4X"},"outputs":[],"source":["trainingloss=pd.DataFrame(data=loss_train)\n","trainingloss.to_csv('/content/mydrive/MyDrive/pj2/outputs/exp3/trainingloss.csv',encoding='utf-8')\n","\n","trainingacc=pd.DataFrame(data=train_acc)\n","trainingacc.to_csv('/content/mydrive/MyDrive/pj2/outputs/exp3/trainingacc.csv',encoding='utf-8')\n","\n","\n","valloss=pd.DataFrame(data=valloss)\n","valloss.to_csv('/content/mydrive/MyDrive/pj2/outputs/exp3/valloss.csv',encoding='utf-8')\n","\n","valacc=pd.DataFrame(data=test_acc)\n","valacc.to_csv('/content/mydrive/MyDrive/pj2/outputs/exp3/testacc.csv',encoding='utf-8')\n","\n","valacc5=pd.DataFrame(data=test_5acc)\n","valacc5.to_csv('/content/mydrive/MyDrive/pj2/outputs/exp3/valacc5.csv',encoding='utf-8')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"TransformersCCTMixup.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"20480f042ae6490c99a16b3dfc117a16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19947ff060540b18bffed422d78db7b","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_331429c5b9524188b86b8dab05386470","value":170498071}},"331429c5b9524188b86b8dab05386470":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"812c740aea284bda9a65870f038428da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c41b795f78443db28b029c283a6d15","placeholder":"​","style":"IPY_MODEL_fa9ca7565e294aae84c606c6c2aeb90b","value":" 170499072/? [00:03\u0026lt;00:00, 54248565.96it/s]"}},"8d485b167c0541b28a2e6a24c9fb310f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0dd346a445e4835a15a6251519791d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20480f042ae6490c99a16b3dfc117a16","IPY_MODEL_812c740aea284bda9a65870f038428da"],"layout":"IPY_MODEL_8d485b167c0541b28a2e6a24c9fb310f"}},"d19947ff060540b18bffed422d78db7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9c41b795f78443db28b029c283a6d15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa9ca7565e294aae84c606c6c2aeb90b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}